scrapy框架的工作原理
    Scrapy Engin（引擎）：scrapy框架的核心部分，负责在spider和item pipeline、Downloader、scheduler中间通信、传递数据等
    Spider（爬虫）：发送需要爬取的连接给引擎，最后引擎把其它模块请求回来的数据再发送给爬虫，爬虫就去解析想要的数据。
    Scheduler（调度器）：负责接收引擎发送过来的请求，并按照一定的方式进行排列和整理，负责调度请求的顺序等。
    Downloader（下载器）：负责接收引擎过来的下载请求，然后去网络上下载队形的数据再交给引擎。
    Item Pipeline（管道）：负责将爬虫传递过来的数据进行保存
    Downloader Middlewares（下载中间件）：可以扩展下载器和引擎之间通信功能的中间件
    Spider Middlewares（Spider中间件）：可以扩展引擎和爬虫之间通信功能的中间件


创建scrapy框架流程：
    进入到想要创建的目录下
    1、输入：scrapy startproject 项目名  ——创建项目
    2、创建爬虫模板：scrapy genspider 爬虫名 域名  如：scrapy genspider qustory xbiquge.la   ——创建爬虫
    3、编写爬虫代码
    4、编写pipeline
        ·yield的三种数据提交方式
            yield{} 字典      推送给pipeline
            yield item 对象   推送给pipeline
            yield scrap.Request() Request对象，推送给调度器
    5、数据存储


scrapy提取数据的方法
    xpath()：返回选择器列表，它代表由指定的xpath表达式参数选择的节点
    css()：返回选择器列表，它代表由指定css表达式作为参数所选择的节点
    re():返回Unicode字符串列表，当正则表达式被赋予作为参数时提取
    extract():返回一个Unicode字符串以及所选择的数据
    extract_first():返回第一个Unicode字符串以及所选择的数据

setting配置文件：
    BOT_NAME = 'qustory' 爬虫的名字，在使用startproject命令创建时自动被赋值
    CONCURRENT_REQUESTS = 32 并发请求的最大值，默认为16
    DEFAULT_REQUEST_HEADERS 默认的请求头
    DOWNLOAD_DELAY = 1  #下载延时设置
    LOG_ENABLED=False # 默认为True，显示日志信息

下载器中间件
    位于引擎和下载器之间通信的中间件
    作用：
        设置代码，更换请求头等来达到反反爬虫的目的
    编写下载器中间件，需要下载器中实现两个方法：
        process_request(self,request,spider),在请求发送之前会执行
        process_response(self,request,response,spider),数据下载到引擎之前执行
    process_request方法详解：
        功能：
            下载器在发送请求之前调用执行，一般可以在这里设置随机代理IP等
        参数：
            request：发送请求的request对象
            spider：发送请求的spider对象
        返回值：
            ①返回None：scrapy将继续处理request，执行其他中间件的相应方法，知道合适的下载器处理函数被调用。
            ②返回response对象：scrapy将不会调用任何其他的process_request方法，将直接返回这个response对象。
                              已经激活的中间件的process_response()方法则会在每个response返回时被调用。
            ③返回request对象:不再使用之前的request对象去下载数据，而是根据现在返回的request对象返回数据。
            ④：如果该方法中跑出了异常，则会调用process_exception方法
    process_response方法详解
        功能：
            下载器下载的数据到引擎中间会执行的方法
        参数：
            request：发送请求的request对象
            response：被处理的response对象
            spider：spider对象
        返回值：
            返回response对象：会将这个新的response对象传给其它中间件，最终传个爬虫。
            返回request对象：下载器链被切断，返回的request会重新被下载器调度下载。
            如果该方法抛出异常，那么调用requests的errback方法，如果没有指定这个方法，那么会抛出一个异常。

Scrapy动态UA的设置：
    fake-useragent一个可以频繁更换user-agent的防反爬利器

    from fake_useragent import UserAgent
    ua=UserAgent()
    ua.ie #ie浏览器
    ua.msie # Mac浏览器
    ua.chrome #chrome浏览器
    ua.random #随机浏览器

    使用：
        def process_request(self,request,spider):
            request.headers['User-Agent']=UserAgent().random